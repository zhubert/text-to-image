{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Phase 4: Text Conditioning with CLIP\n",
    "\n",
    "In Phase 3, we controlled generation with **discrete class labels** (0-9 for digits). This required:\n",
    "- A fixed vocabulary of classes\n",
    "- A learnable embedding table\n",
    "- Addition to the timestep embedding\n",
    "\n",
    "In this notebook, we move to **natural language conditioning** - the ability to say \"a photo of a cat\" and generate a cat!\n",
    "\n",
    "## The Evolution of Conditioning\n",
    "\n",
    "| Phase | Conditioning | Example | Mechanism |\n",
    "|-------|-------------|---------|------------|\n",
    "| 1-2 | None | Generate any image | - |\n",
    "| 3 | Class label | \"Generate digit 7\" | $c = h_t + e(y)$ |\n",
    "| 4 | Text prompt | \"a photo of a cat\" | Cross-attention to $Z_{\\text{text}}$ |\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "We now learn a text-conditional velocity field:\n",
    "\n",
    "$$v_\\theta(x_t, t, \\text{text}) : \\mathbb{R}^{C \\times H \\times W} \\times [0,1] \\times \\text{String} \\to \\mathbb{R}^{C \\times H \\times W}$$\n",
    "\n",
    "The text string is converted to a sequence of embeddings via CLIP:\n",
    "\n",
    "$$\\text{text} \\xrightarrow{\\text{CLIP}} Z = [z_1, z_2, \\ldots, z_M] \\in \\mathbb{R}^{M \\times D}$$\n",
    "\n",
    "where $M$ is the sequence length and $D$ is the embedding dimension.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **CLIP Text Encoder**: How text becomes vectors\n",
    "2. **Cross-Attention**: How image patches attend to text tokens\n",
    "3. **Text-Conditional DiT**: The full architecture\n",
    "4. **CFG for Text**: Classifier-free guidance with prompts\n",
    "\n",
    "## Why Cross-Attention Instead of Addition?\n",
    "\n",
    "In Phase 3, we simply added the class embedding to the timestep embedding:\n",
    "\n",
    "$$c = h_t + e(y) \\in \\mathbb{R}^D$$\n",
    "\n",
    "This works for **single tokens** (class labels), but text has **multiple tokens**. We need a mechanism that:\n",
    "\n",
    "1. Handles variable-length sequences\n",
    "2. Allows different image regions to focus on different words\n",
    "3. Preserves the full information in each token\n",
    "\n",
    "**Cross-attention** provides all of this:\n",
    "\n",
    "$$\\text{CrossAttn}(X, Z) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "where:\n",
    "- $Q = X W_Q$ (queries from image patches)\n",
    "- $K = Z W_K$ (keys from text tokens)\n",
    "- $V = Z W_V$ (values from text tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Auto-reload modules during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up device\n",
    "from text_to_image import get_device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clip-intro",
   "metadata": {},
   "source": [
    "## 1. CLIP: The Bridge Between Text and Images\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) is a model trained on 400M image-text pairs to learn a shared embedding space where:\n",
    "\n",
    "$$\\text{sim}(f_{\\text{image}}(I), f_{\\text{text}}(T)) \\text{ is high when } I \\text{ matches } T$$\n",
    "\n",
    "### The CLIP Training Objective\n",
    "\n",
    "Given a batch of $N$ image-text pairs, CLIP learns to:\n",
    "\n",
    "1. **Maximize** similarity between matching pairs (diagonal)\n",
    "2. **Minimize** similarity between non-matching pairs (off-diagonal)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{CLIP}} = -\\frac{1}{N}\\sum_{i=1}^N \\log \\frac{\\exp(\\text{sim}(I_i, T_i)/\\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(I_i, T_j)/\\tau)}$$\n",
    "\n",
    "This is a symmetric cross-entropy loss over the similarity matrix.\n",
    "\n",
    "### Why CLIP Works for Conditioning\n",
    "\n",
    "The key insight: CLIP embeddings are **visually grounded**.\n",
    "\n",
    "| Property | Meaning | Example |\n",
    "|----------|---------|----------|\n",
    "| Semantic | Similar meanings → similar vectors | \"dog\" ≈ \"puppy\" |\n",
    "| Visual | Vectors encode visual features | \"red\" encodes color info |\n",
    "| Compositional | Combinations work | \"blue dog\" is meaningful |\n",
    "\n",
    "### CLIP Architecture\n",
    "\n",
    "```\n",
    "Text: \"a photo of a cat\"\n",
    "         |\n",
    "         v\n",
    "    +---------+\n",
    "    | Tokenize |  \"a\", \"photo\", \"of\", \"a\", \"cat\"\n",
    "    +---------+\n",
    "         |\n",
    "         v\n",
    "    +---------+\n",
    "    | Embed   |  [e_1, e_2, e_3, e_4, e_5]\n",
    "    +---------+\n",
    "         |\n",
    "         v\n",
    "    +---------+\n",
    "    |Transformer|  12 layers of self-attention\n",
    "    +---------+\n",
    "         |\n",
    "         v\n",
    "    Token Embeddings: Z ∈ R^{M × D}\n",
    "         |\n",
    "         v\n",
    "    Pooled (EOS token): z_pool ∈ R^D\n",
    "```\n",
    "\n",
    "We use the **token embeddings** (not just the pooled output) for cross-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-clip",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.text_encoder import CLIPTextEncoder\n",
    "\n",
    "# Load CLIP text encoder\n",
    "# This downloads the model on first run (~250MB for ViT-B/32)\n",
    "text_encoder = CLIPTextEncoder(\n",
    "    model_name=\"openai/clip-vit-base-patch32\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"CLIP embedding dimension: {text_encoder.embed_dim}\")\n",
    "print(f\"Max sequence length: {text_encoder.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-clip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore CLIP embeddings\n",
    "prompts = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a car\",\n",
    "    \"a furry cat sitting\",\n",
    "    \"a fluffy dog running\",\n",
    "]\n",
    "\n",
    "# Get token embeddings and pooled embeddings\n",
    "token_embeddings, pooled = text_encoder.encode(prompts, return_pooled=True)\n",
    "\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "print(f\"  → (batch_size, max_length, embed_dim)\")\n",
    "print(f\"Pooled embeddings shape: {pooled.shape}\")\n",
    "print(f\"  → (batch_size, embed_dim)\")\n",
    "\n",
    "# Compute similarity matrix from pooled embeddings\n",
    "pooled_norm = pooled / pooled.norm(dim=-1, keepdim=True)\n",
    "similarity = (pooled_norm @ pooled_norm.T).cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(similarity, cmap='viridis', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(prompts)))\n",
    "ax.set_yticks(range(len(prompts)))\n",
    "ax.set_xticklabels([p[:15] + \"...\" for p in prompts], rotation=45, ha='right')\n",
    "ax.set_yticklabels([p[:15] + \"...\" for p in prompts])\n",
    "ax.set_title('CLIP Embedding Similarity')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  - 'cat' prompts are similar to each other\")\n",
    "print(\"  - 'dog' prompts are similar to each other\")\n",
    "print(\"  - Both are less similar to 'car'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-attn-math",
   "metadata": {},
   "source": [
    "## 2. Cross-Attention: Image Patches Attending to Text\n",
    "\n",
    "Cross-attention is the mechanism that allows each image patch to \"look at\" the text and extract relevant information.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given:\n",
    "- Image patch embeddings: $X \\in \\mathbb{R}^{N \\times d}$ (N patches)\n",
    "- Text token embeddings: $Z \\in \\mathbb{R}^{M \\times D}$ (M tokens)\n",
    "\n",
    "Cross-attention computes:\n",
    "\n",
    "$$\\text{CrossAttn}(X, Z) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where:\n",
    "- $Q = X W_Q \\in \\mathbb{R}^{N \\times d_k}$ — **Queries** from image patches\n",
    "- $K = Z W_K \\in \\mathbb{R}^{M \\times d_k}$ — **Keys** from text tokens  \n",
    "- $V = Z W_V \\in \\mathbb{R}^{M \\times d_v}$ — **Values** from text tokens\n",
    "\n",
    "### The Attention Matrix\n",
    "\n",
    "$$A = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{N \\times M}$$\n",
    "\n",
    "Entry $A_{ij}$ represents: \"How much should image patch $i$ attend to text token $j$?\"\n",
    "\n",
    "### Intuitive Understanding\n",
    "\n",
    "For the prompt \"a RED dog RUNNING\":\n",
    "\n",
    "```\n",
    "                    Text Tokens\n",
    "                 a   RED  dog  RUNNING\n",
    "              ┌────────────────────────┐\n",
    "    Patch 1   │ 0.1  0.7  0.1   0.1    │  ← Color region, attends to \"RED\"\n",
    "    Patch 2   │ 0.1  0.1  0.6   0.2    │  ← Body region, attends to \"dog\"\n",
    "    Patch 3   │ 0.1  0.1  0.3   0.5    │  ← Leg region, attends to \"RUNNING\"\n",
    "    Patch 4   │ 0.2  0.3  0.3   0.2    │  ← Background, mixed attention\n",
    "              └────────────────────────┘\n",
    "                    Attention Matrix A\n",
    "```\n",
    "\n",
    "### Comparison: Self-Attention vs Cross-Attention\n",
    "\n",
    "| Aspect | Self-Attention | Cross-Attention |\n",
    "|--------|----------------|------------------|\n",
    "| Q, K, V source | Same sequence (X) | Q from X, K/V from Z |\n",
    "| Attention shape | $N \\times N$ | $N \\times M$ |\n",
    "| Purpose | Patch-to-patch relations | Text-to-image transfer |\n",
    "| Information flow | Within image | From text to image |\n",
    "\n",
    "### Multi-Head Cross-Attention\n",
    "\n",
    "We use multiple attention heads, each learning different aspects:\n",
    "\n",
    "$$\\text{MultiHead}(X, Z) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_H) W_O$$\n",
    "\n",
    "where:\n",
    "$$\\text{head}_i = \\text{CrossAttn}(X W_Q^i, Z W_K^i, Z W_V^i)$$\n",
    "\n",
    "Different heads might learn:\n",
    "- Head 1: Color information (\"red\", \"blue\")\n",
    "- Head 2: Object identity (\"cat\", \"dog\")\n",
    "- Head 3: Actions/poses (\"running\", \"sitting\")\n",
    "- Head 4: Spatial relationships (\"on\", \"next to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-cross-attn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.dit import CrossAttention\n",
    "\n",
    "# Create a cross-attention layer\n",
    "embed_dim = 256   # Image patch embedding dimension\n",
    "context_dim = 512  # CLIP text embedding dimension\n",
    "num_heads = 8\n",
    "\n",
    "cross_attn = CrossAttention(\n",
    "    embed_dim=embed_dim,\n",
    "    context_dim=context_dim,\n",
    "    num_heads=num_heads,\n",
    ").to(device)\n",
    "\n",
    "# Simulate inputs\n",
    "batch_size = 2\n",
    "num_patches = 64  # 8x8 grid\n",
    "num_tokens = 10   # Text sequence length\n",
    "\n",
    "# Random image patch embeddings\n",
    "x = torch.randn(batch_size, num_patches, embed_dim, device=device)\n",
    "\n",
    "# Random text embeddings (simulating CLIP output)\n",
    "context = torch.randn(batch_size, num_tokens, context_dim, device=device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = cross_attn(x, context)\n",
    "\n",
    "print(f\"Cross-Attention Dimensions:\")\n",
    "print(f\"  Image patches (X): {x.shape}\")\n",
    "print(f\"  Text tokens (Z):   {context.shape}\")\n",
    "print(f\"  Output:            {output.shape}\")\n",
    "print(f\"\\nThe output has the same shape as the image patches!\")\n",
    "print(f\"Each patch now contains information from relevant text tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights for a real prompt\n",
    "def visualize_cross_attention_weights(\n",
    "    cross_attn: CrossAttention,\n",
    "    x: torch.Tensor,\n",
    "    context: torch.Tensor,\n",
    "    tokens: list[str],\n",
    "):\n",
    "    \"\"\"Visualize what each image patch attends to in the text.\"\"\"\n",
    "    B, N, _ = x.shape\n",
    "    M = context.shape[1]\n",
    "    H = cross_attn.num_heads\n",
    "    d = cross_attn.head_dim\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute Q, K\n",
    "        q = cross_attn.q_proj(x)\n",
    "        k = cross_attn.k_proj(context)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        q = q.view(B, N, H, d).transpose(1, 2)\n",
    "        k = k.view(B, M, H, d).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * cross_attn.scale\n",
    "        attn = attn.softmax(dim=-1)  # (B, H, N, M)\n",
    "    \n",
    "    # Average over heads and batch\n",
    "    attn_avg = attn[0].mean(dim=0).cpu().numpy()  # (N, M)\n",
    "    \n",
    "    # Reshape to 2D grid (assuming square patches)\n",
    "    grid_size = int(np.sqrt(N))\n",
    "    \n",
    "    # Plot attention for each token\n",
    "    num_tokens_to_show = min(6, M)\n",
    "    fig, axes = plt.subplots(1, num_tokens_to_show, figsize=(3*num_tokens_to_show, 3))\n",
    "    \n",
    "    for i in range(num_tokens_to_show):\n",
    "        attn_map = attn_avg[:, i].reshape(grid_size, grid_size)\n",
    "        axes[i].imshow(attn_map, cmap='hot')\n",
    "        token = tokens[i] if i < len(tokens) else \"[PAD]\"\n",
    "        axes[i].set_title(f'\"{token}\"')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Cross-Attention: Which patches attend to which tokens?', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get real CLIP embeddings\n",
    "test_prompt = \"a red car driving fast\"\n",
    "test_embeddings, test_mask = text_encoder(test_prompt)\n",
    "test_embeddings = test_embeddings.to(device)\n",
    "\n",
    "# Get tokens for visualization\n",
    "tokens = text_encoder.tokenizer.tokenize(test_prompt)\n",
    "tokens = ['[BOS]'] + tokens + ['[EOS]']\n",
    "\n",
    "# Random image patches (in practice, these would be from the DiT)\n",
    "num_patches = 64\n",
    "x_test = torch.randn(1, num_patches, embed_dim, device=device)\n",
    "\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print()\n",
    "\n",
    "visualize_cross_attention_weights(cross_attn, x_test, test_embeddings, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "## 3. Text-Conditional DiT Architecture\n",
    "\n",
    "The TextConditionalDiT extends our Phase 2 DiT with cross-attention layers.\n",
    "\n",
    "### Block Structure\n",
    "\n",
    "Each TextConditionedDiTBlock has three components:\n",
    "\n",
    "```\n",
    "                                        Text Embeddings Z\n",
    "                                              |\n",
    "                                              v\n",
    "x --> adaLN --> Self-Attn --> + --> adaLN --> Cross-Attn --> + --> adaLN --> MLP --> + --> out\n",
    "|                             |   |                          |   |                   |\n",
    "+-------(residual)------------+   +-------(residual)---------+   +----(residual)-----+\n",
    "```\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let $x^{(l)}$ be the input to layer $l$, $c$ be timestep conditioning, $Z$ be text embeddings:\n",
    "\n",
    "$$x^{(l+1/3)} = x^{(l)} + \\text{SelfAttn}(\\text{adaLN}(x^{(l)}, c))$$\n",
    "$$x^{(l+2/3)} = x^{(l+1/3)} + \\text{CrossAttn}(\\text{adaLN}(x^{(l+1/3)}, c), Z)$$\n",
    "$$x^{(l+1)} = x^{(l+2/3)} + \\text{MLP}(\\text{adaLN}(x^{(l+2/3)}, c))$$\n",
    "\n",
    "### Full Architecture\n",
    "\n",
    "```\n",
    "Text Prompt                    Noisy Image x_t          Timestep t\n",
    "     |                              |                       |\n",
    "     v                              v                       v\n",
    "+---------+                  +-----------+           +-----------+\n",
    "|  CLIP   |                  | Patchify  |           | Time Emb  |\n",
    "| Encoder |                  | + PosEmb  |           |   (MLP)   |\n",
    "+----+----+                  +-----+-----+           +-----+-----+\n",
    "     |                             |                       |\n",
    "     | Z ∈ R^{M × D}               | X ∈ R^{N × d}         | c ∈ R^{d_c}\n",
    "     |                             |                       |\n",
    "     |                             v                       |\n",
    "     |                    +-----------------+              |\n",
    "     +------------------->| TextConditioned |<-------------+\n",
    "                          |    DiT Blocks   |\n",
    "                          +--------+--------+\n",
    "                                   |\n",
    "                                   v\n",
    "                          +-----------------+\n",
    "                          |   Unpatchify    |\n",
    "                          +--------+--------+\n",
    "                                   |\n",
    "                                   v\n",
    "                          Velocity v ∈ R^{C × H × W}\n",
    "```\n",
    "\n",
    "### Key Differences from ConditionalDiT\n",
    "\n",
    "| Aspect | ConditionalDiT | TextConditionalDiT |\n",
    "|--------|----------------|--------------------|\n",
    "| Conditioning | Class labels (0-9) | Text prompts |\n",
    "| Embedding | Learnable table | Frozen CLIP |\n",
    "| Integration | Added to timestep | Cross-attention |\n",
    "| Vocabulary | Fixed (10 classes) | Open (any text) |\n",
    "| Representation | Single vector | Sequence of tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.dit import TextConditionalDiT\n",
    "\n",
    "# Create the text-conditional DiT for CIFAR-10 (32x32 RGB)\n",
    "model = TextConditionalDiT(\n",
    "    img_size=32,          # CIFAR-10 image size\n",
    "    patch_size=4,         # 4x4 patches → 8x8 = 64 patches\n",
    "    in_channels=3,        # RGB\n",
    "    embed_dim=256,        # Patch embedding dimension\n",
    "    depth=6,              # Number of transformer blocks\n",
    "    num_heads=8,          # Attention heads\n",
    "    mlp_ratio=4.0,        # MLP expansion\n",
    "    context_dim=512,      # CLIP ViT-B/32 embedding dimension\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"TextConditionalDiT Parameters: {num_params:,}\")\n",
    "\n",
    "# Compare to ConditionalDiT\n",
    "from text_to_image.dit import ConditionalDiT\n",
    "class_model = ConditionalDiT(img_size=32, patch_size=4, in_channels=3)\n",
    "class_params = sum(p.numel() for p in class_model.parameters() if p.requires_grad)\n",
    "print(f\"ConditionalDiT Parameters: {class_params:,}\")\n",
    "print(f\"Difference: +{num_params - class_params:,} (cross-attention layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "batch_size = 4\n",
    "\n",
    "# Random noisy images\n",
    "x = torch.randn(batch_size, 3, 32, 32, device=device)\n",
    "\n",
    "# Random timesteps\n",
    "t = torch.rand(batch_size, device=device)\n",
    "\n",
    "# Text embeddings from CLIP\n",
    "prompts = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a car\",\n",
    "    \"a photo of a plane\",\n",
    "]\n",
    "text_embeddings, text_mask = text_encoder(prompts)\n",
    "text_embeddings = text_embeddings.to(device)\n",
    "text_mask = text_mask.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    v_pred = model(x, t, text_embeddings, text_mask)\n",
    "\n",
    "print(f\"Input shape:       {x.shape}\")\n",
    "print(f\"Timesteps shape:   {t.shape}\")\n",
    "print(f\"Text embed shape:  {text_embeddings.shape}\")\n",
    "print(f\"Text mask shape:   {text_mask.shape}\")\n",
    "print(f\"Output shape:      {v_pred.shape}\")\n",
    "print(f\"\\nModel correctly outputs velocity field with same shape as input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": [
    "## 4. Dataset: CIFAR-10 with Text Captions\n",
    "\n",
    "We'll use CIFAR-10 (32×32 RGB images) with class names converted to captions.\n",
    "\n",
    "### CIFAR-10 Classes\n",
    "\n",
    "| Index | Class | Caption Template |\n",
    "|-------|-------|------------------|\n",
    "| 0 | airplane | \"a photo of an airplane\" |\n",
    "| 1 | automobile | \"a photo of an automobile\" |\n",
    "| 2 | bird | \"a photo of a bird\" |\n",
    "| 3 | cat | \"a photo of a cat\" |\n",
    "| 4 | deer | \"a photo of a deer\" |\n",
    "| 5 | dog | \"a photo of a dog\" |\n",
    "| 6 | frog | \"a photo of a frog\" |\n",
    "| 7 | horse | \"a photo of a horse\" |\n",
    "| 8 | ship | \"a photo of a ship\" |\n",
    "| 9 | truck | \"a photo of a truck\" |\n",
    "\n",
    "### Why Simple Captions?\n",
    "\n",
    "- CLIP understands \"a photo of a X\" format very well\n",
    "- Consistent structure helps training\n",
    "- Easy to extend with adjectives: \"a photo of a red car\"\n",
    "\n",
    "In production, you'd use actual image captions from datasets like COCO or LAION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.text_encoder import make_cifar10_captions\n",
    "\n",
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Test caption function\n",
    "sample_labels = torch.tensor([0, 3, 5, 8])\n",
    "captions = make_cifar10_captions(sample_labels)\n",
    "\n",
    "print(\"Label → Caption mapping:\")\n",
    "for label, caption in zip(sample_labels.tolist(), captions):\n",
    "    print(f\"  {label} → '{caption}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples with their captions\n",
    "images, labels = next(iter(train_loader))\n",
    "captions = make_cifar10_captions(labels[:8])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "for i, (ax, img, caption) in enumerate(zip(axes.flat, images[:8], captions)):\n",
    "    # Denormalize\n",
    "    img = (img + 1) / 2\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(caption, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('CIFAR-10 Samples with Text Captions', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfg-text",
   "metadata": {},
   "source": [
    "## 5. Classifier-Free Guidance for Text\n",
    "\n",
    "CFG works the same way as in Phase 3, but with text embeddings instead of class embeddings.\n",
    "\n",
    "### The CFG Formula (Unchanged)\n",
    "\n",
    "$$v_{\\text{CFG}} = v_{\\text{uncond}} + w \\cdot (v_{\\text{cond}} - v_{\\text{uncond}})$$\n",
    "\n",
    "where:\n",
    "- $v_{\\text{cond}} = v_\\theta(x_t, t, Z_{\\text{text}})$ — Velocity with text embedding\n",
    "- $v_{\\text{uncond}} = v_\\theta(x_t, t, Z_{\\text{null}})$ — Velocity with null text\n",
    "- $w$ — Guidance scale\n",
    "\n",
    "### What is Null Text?\n",
    "\n",
    "The \"null text\" is simply an empty string \"\" encoded by CLIP:\n",
    "\n",
    "$$Z_{\\text{null}} = \\text{CLIP}(\"\") \\in \\mathbb{R}^{M \\times D}$$\n",
    "\n",
    "This gives us an embedding that represents \"no specific content\" — similar to the learned null class in Phase 3.\n",
    "\n",
    "### Text Dropout During Training\n",
    "\n",
    "To enable CFG, we randomly replace text embeddings with null during training:\n",
    "\n",
    "$$Z_{\\text{train}} = \\begin{cases}\n",
    "\\text{CLIP}(\\text{prompt}) & \\text{with probability } 1 - p_{\\text{drop}} \\\\\n",
    "\\text{CLIP}(\"\") & \\text{with probability } p_{\\text{drop}}\n",
    "\\end{cases}$$\n",
    "\n",
    "Typically $p_{\\text{drop}} = 0.1$ (10%).\n",
    "\n",
    "### Guidance Scale for Text vs Class\n",
    "\n",
    "Text-conditional models typically use **higher** CFG scales:\n",
    "\n",
    "| Conditioning | Typical Scale | Reason |\n",
    "|--------------|---------------|--------|\n",
    "| Class (Phase 3) | 3-5 | Simple, direct mapping |\n",
    "| Text (Phase 4) | 7-10 | Complex, needs stronger push |\n",
    "\n",
    "Why? Text conditioning is more nuanced — there are many ways to satisfy \"a photo of a cat\". Higher guidance pushes toward the most \"cat-like\" outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-null-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text embedding vs null embedding\n",
    "text_emb, _ = text_encoder.encode([\"a photo of a cat\"], return_pooled=True)\n",
    "null_emb, _ = text_encoder.encode([\"\"], return_pooled=True)\n",
    "\n",
    "# Compute similarity\n",
    "text_norm = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "null_norm = null_emb / null_emb.norm(dim=-1, keepdim=True)\n",
    "similarity = (text_norm @ null_norm.T).item()\n",
    "\n",
    "print(f\"Text embedding shape: {text_emb.shape}\")\n",
    "print(f\"Null embedding shape: {null_emb.shape}\")\n",
    "print(f\"Cosine similarity: {similarity:.3f}\")\n",
    "print(f\"\\nThe null embedding is quite different from any real text embedding.\")\n",
    "print(f\"This makes CFG work: we can push away from 'nothing' toward 'something'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## 6. Training the Text-Conditional DiT\n",
    "\n",
    "Training follows the same flow matching objective, now with text conditioning.\n",
    "\n",
    "### Training Algorithm\n",
    "\n",
    "For each batch of (image, label) pairs:\n",
    "\n",
    "1. **Convert labels to captions**: $y \\to \\text{\"a photo of a \"} + \\text{class\\_name}$\n",
    "2. **Encode with CLIP**: $\\text{caption} \\to Z \\in \\mathbb{R}^{M \\times D}$\n",
    "3. **Apply text dropout**: With 10% probability, $Z \\leftarrow Z_{\\text{null}}$\n",
    "4. **Sample noise and time**: $x_1 \\sim \\mathcal{N}(0, I)$, $t \\sim \\text{Uniform}(0, 1)$\n",
    "5. **Interpolate**: $x_t = (1-t) x_0 + t x_1$\n",
    "6. **Predict velocity**: $\\hat{v} = v_\\theta(x_t, t, Z)$\n",
    "7. **Compute loss**: $\\mathcal{L} = \\|\\hat{v} - (x_1 - x_0)\\|^2$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, x_1, t, Z}\\left[ \\| v_\\theta(x_t, t, Z) - (x_1 - x_0) \\|^2 \\right]$$\n",
    "\n",
    "where $Z$ may be the real text embedding or null embedding (with probability $p_{\\text{drop}}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.train import TextConditionalTrainer\n",
    "\n",
    "# Create the trainer\n",
    "trainer = TextConditionalTrainer(\n",
    "    model=model,\n",
    "    text_encoder=text_encoder,\n",
    "    dataloader=train_loader,\n",
    "    caption_fn=make_cifar10_captions,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    text_drop_prob=0.1,  # 10% text dropout for CFG\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Training Text-Conditional DiT with CFG text dropout...\")\n",
    "print(\"(10% of samples trained with null text)\\n\")\n",
    "\n",
    "# Train for a few epochs (increase for better results)\n",
    "NUM_EPOCHS = 30  # Increase to 50-100 for better quality\n",
    "losses = trainer.train(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Text-Conditional DiT Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sampling-section",
   "metadata": {},
   "source": [
    "## 7. Sampling with Text Prompts\n",
    "\n",
    "Now the exciting part — generating images from text!\n",
    "\n",
    "### CFG Sampling Algorithm\n",
    "\n",
    "```\n",
    "Input: Trained model v_θ, text prompt, guidance scale w, num_steps N\n",
    "\n",
    "1. Encode text: Z_text = CLIP(prompt)\n",
    "2. Encode null: Z_null = CLIP(\"\")\n",
    "3. Sample x_1 ~ N(0, I)  # Pure noise\n",
    "4. dt = 1/N\n",
    "\n",
    "For t = 1, 1-dt, ..., dt:\n",
    "    v_cond = v_θ(x_t, t, Z_text)    # With text\n",
    "    v_uncond = v_θ(x_t, t, Z_null)  # Without text\n",
    "    v_guided = v_uncond + w × (v_cond - v_uncond)  # CFG\n",
    "    x_t ← x_t - dt × v_guided       # Euler step\n",
    "\n",
    "Return x_0  # Generated image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.sampling import sample_text_conditional\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Generate images for each CIFAR-10 class\n",
    "prompts = [\n",
    "    \"a photo of an airplane\",\n",
    "    \"a photo of an automobile\",\n",
    "    \"a photo of a bird\",\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a deer\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a frog\",\n",
    "    \"a photo of a horse\",\n",
    "    \"a photo of a ship\",\n",
    "    \"a photo of a truck\",\n",
    "]\n",
    "\n",
    "print(\"Generating images for each class with CFG scale=7.5...\")\n",
    "\n",
    "samples = sample_text_conditional(\n",
    "    model=model,\n",
    "    text_encoder=text_encoder,\n",
    "    prompts=prompts,\n",
    "    image_shape=(3, 32, 32),\n",
    "    num_steps=50,\n",
    "    cfg_scale=7.5,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i, (ax, prompt) in enumerate(zip(axes.flat, prompts)):\n",
    "    img = (samples[i] + 1) / 2  # Denormalize\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(prompt.replace('a photo of ', ''), fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Text-to-Image Generation (CFG=7.5)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different CFG scales\n",
    "test_prompt = \"a photo of a cat\"\n",
    "cfg_scales = [1.0, 3.0, 5.0, 7.5, 10.0, 15.0]\n",
    "\n",
    "print(f\"Comparing CFG scales for: '{test_prompt}'\")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cfg_scales), figsize=(18, 3))\n",
    "\n",
    "for ax, scale in zip(axes, cfg_scales):\n",
    "    # Use same seed for fair comparison\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    sample = sample_text_conditional(\n",
    "        model=model,\n",
    "        text_encoder=text_encoder,\n",
    "        prompts=[test_prompt],\n",
    "        image_shape=(3, 32, 32),\n",
    "        num_steps=50,\n",
    "        cfg_scale=scale,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    img = (sample[0] + 1) / 2\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'scale={scale}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of CFG Scale: \"{test_prompt}\"', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  scale=1.0: No guidance, blurry\")\n",
    "print(\"  scale=3-5: Mild guidance, improving\")\n",
    "print(\"  scale=7-10: Good balance\")\n",
    "print(\"  scale=15+: May oversaturate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some creative prompts (may or may not work well on CIFAR-10)\n",
    "creative_prompts = [\n",
    "    \"a photo of a flying car\",\n",
    "    \"a photo of a cat on a ship\",\n",
    "    \"a photo of a red bird\",\n",
    "    \"a photo of a fast truck\",\n",
    "]\n",
    "\n",
    "print(\"Testing compositional understanding...\")\n",
    "print(\"(Results may be limited by CIFAR-10 training data)\\n\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "creative_samples = sample_text_conditional(\n",
    "    model=model,\n",
    "    text_encoder=text_encoder,\n",
    "    prompts=creative_prompts,\n",
    "    image_shape=(3, 32, 32),\n",
    "    num_steps=50,\n",
    "    cfg_scale=7.5,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, prompt, sample in zip(axes, creative_prompts, creative_samples):\n",
    "    img = (sample + 1) / 2\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(prompt, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Compositional Prompts', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: Key Mathematical Concepts\n",
    "\n",
    "### What We Built\n",
    "\n",
    "We extended the DiT to accept natural language prompts via CLIP and cross-attention.\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "| Concept | Equation |\n",
    "|---------|----------|\n",
    "| CLIP encoding | $Z = \\text{CLIP}(\\text{text}) \\in \\mathbb{R}^{M \\times D}$ |\n",
    "| Cross-attention | $\\text{CrossAttn}(X, Z) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V$ |\n",
    "| Queries | $Q = X W_Q$ (from image patches) |\n",
    "| Keys, Values | $K = Z W_K$, $V = Z W_V$ (from text tokens) |\n",
    "| Block output | $x' = x + \\text{SelfAttn}(x) + \\text{CrossAttn}(x, Z) + \\text{MLP}(x)$ |\n",
    "| Training loss | $\\mathcal{L} = \\mathbb{E}[\\|v_\\theta(x_t, t, Z) - (x_1 - x_0)\\|^2]$ |\n",
    "| CFG formula | $v_{\\text{CFG}} = v_{\\text{uncond}} + w \\cdot (v_{\\text{cond}} - v_{\\text{uncond}})$ |\n",
    "| Null text | $Z_{\\text{null}} = \\text{CLIP}(\"\")$ |\n",
    "\n",
    "### Cross-Attention Mathematics\n",
    "\n",
    "The attention matrix $A \\in \\mathbb{R}^{N \\times M}$ where:\n",
    "- $N$ = number of image patches\n",
    "- $M$ = number of text tokens\n",
    "\n",
    "$$A_{ij} = \\frac{\\exp(q_i \\cdot k_j / \\sqrt{d})}{\\sum_{j'} \\exp(q_i \\cdot k_{j'} / \\sqrt{d})}$$\n",
    "\n",
    "$A_{ij}$ answers: \"How much should image patch $i$ attend to text token $j$?\"\n",
    "\n",
    "### Architecture Comparison Across Phases\n",
    "\n",
    "| Phase | Model | Conditioning | Mechanism |\n",
    "|-------|-------|-------------|------------|\n",
    "| 1-2 | DiT | None | - |\n",
    "| 3 | ConditionalDiT | Class labels | $c = h_t + e(y)$, adaLN |\n",
    "| 4 | TextConditionalDiT | Text prompts | Cross-attention to CLIP embeddings |\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "| Aspect | Phase 3 | Phase 4 |\n",
    "|--------|---------|----------|\n",
    "| Encoder | Learnable (11×1024 params) | Frozen CLIP (~150M params) |\n",
    "| New layers | None | Cross-attention in each block |\n",
    "| CFG forward passes | 2 per step | 2 per step |\n",
    "| Guidance scale | 3-5 typical | 7-10 typical |\n",
    "\n",
    "## Looking Ahead: Phase 5\n",
    "\n",
    "In Phase 5 (optional), we'll add **latent space diffusion** with a VAE:\n",
    "\n",
    "| Current (Pixel Space) | Phase 5 (Latent Space) |\n",
    "|----------------------|------------------------|\n",
    "| Diffusion on 32×32×3 | Diffusion on 4×4×4 latent |\n",
    "| 3,072 dimensions | 64 dimensions (48× smaller) |\n",
    "| Limited resolution | Scales to 256×256+ |\n",
    "\n",
    "This is how Stable Diffusion achieves high-resolution generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the trained model\n",
    "# trainer.save_checkpoint(\"phase4_text_conditional_dit.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
