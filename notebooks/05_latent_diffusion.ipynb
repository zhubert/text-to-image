{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Phase 5: Latent Diffusion with VAE\n",
    "\n",
    "In previous phases, we performed flow matching directly in **pixel space**. While this works well for small images (MNIST 28×28, CIFAR-10 32×32), it doesn't scale to high-resolution images.\n",
    "\n",
    "## The Scaling Problem\n",
    "\n",
    "Consider the computational cost of pixel-space diffusion:\n",
    "\n",
    "| Resolution | Dimensions | Relative Cost |\n",
    "|------------|------------|---------------|\n",
    "| 32×32×3 | 3,072 | 1× |\n",
    "| 64×64×3 | 12,288 | 4× |\n",
    "| 256×256×3 | 196,608 | 64× |\n",
    "| 512×512×3 | 786,432 | 256× |\n",
    "\n",
    "Transformer self-attention has O(N²) complexity in sequence length. For images:\n",
    "- 32×32 with 4×4 patches = 64 tokens → 4,096 attention pairs\n",
    "- 256×256 with 4×4 patches = 4,096 tokens → 16,777,216 attention pairs\n",
    "\n",
    "This is **4,096× more computation** just for attention!\n",
    "\n",
    "## The Solution: Latent Diffusion\n",
    "\n",
    "The key insight of **Stable Diffusion** (Rombach et al., 2022):\n",
    "\n",
    "> Instead of diffusing in pixel space, first compress images to a smaller latent space, then do diffusion there.\n",
    "\n",
    "```\n",
    "Pixel Space Diffusion:\n",
    "  noise (256×256×3) ──flow matching──> image (256×256×3)\n",
    "  \n",
    "Latent Space Diffusion:\n",
    "  noise (32×32×4) ──flow matching──> latent (32×32×4) ──VAE decode──> image (256×256×3)\n",
    "```\n",
    "\n",
    "With 8× spatial compression:\n",
    "- 256×256×3 image → 32×32×4 latent\n",
    "- 196,608 → 4,096 dimensions (48× smaller!)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Variational Autoencoders (VAEs)**: How to compress images to latent space\n",
    "2. **The Reparameterization Trick**: How to backpropagate through sampling\n",
    "3. **Latent Flow Matching**: Training diffusion in compressed space\n",
    "4. **The Full Pipeline**: Encode → Denoise → Decode\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "### VAE: Learning a Latent Space\n",
    "\n",
    "A VAE learns two functions:\n",
    "\n",
    "**Encoder**: $q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma^2_\\phi(x))$\n",
    "- Maps image $x$ to a distribution over latents $z$\n",
    "- Outputs mean $\\mu$ and variance $\\sigma^2$ of a Gaussian\n",
    "\n",
    "**Decoder**: $p_\\theta(x|z)$\n",
    "- Maps latent $z$ back to image $x$\n",
    "- Learns to reconstruct the original image\n",
    "\n",
    "### Training Objective (ELBO)\n",
    "\n",
    "The VAE is trained to maximize the Evidence Lower Bound:\n",
    "\n",
    "$$\\log p(x) \\geq \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))$$\n",
    "\n",
    "In practice, we minimize:\n",
    "\n",
    "$$\\mathcal{L}_{VAE} = \\underbrace{\\|x - \\text{decode}(\\text{encode}(x))\\|^2}_{\\text{reconstruction}} + \\beta \\cdot \\underbrace{D_{KL}(q(z|x) \\| \\mathcal{N}(0,I))}_{\\text{regularization}}$$\n",
    "\n",
    "### Latent Diffusion\n",
    "\n",
    "Once the VAE is trained, we do flow matching in latent space:\n",
    "\n",
    "1. **Encode training data**: $z_0 = \\text{encode}(x)$\n",
    "2. **Flow matching on latents**: Learn $v_\\theta(z_t, t)$ where $z_t = (1-t)z_0 + tz_1$\n",
    "3. **Generate**: Sample $z_1 \\sim \\mathcal{N}(0,I)$, integrate to $z_0$, decode to $x = \\text{decode}(z_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Auto-reload modules during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up device\n",
    "from text_to_image import get_device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vae-intro",
   "metadata": {},
   "source": [
    "## 1. Variational Autoencoders: The Compression Engine\n",
    "\n",
    "### Why VAE Instead of Regular Autoencoder?\n",
    "\n",
    "A regular autoencoder learns a deterministic mapping:\n",
    "$$x \\xrightarrow{\\text{encode}} z \\xrightarrow{\\text{decode}} \\hat{x}$$\n",
    "\n",
    "The problem: the latent space may have \"holes\" - regions where no training data maps to, producing garbage when decoded.\n",
    "\n",
    "A VAE learns a **probabilistic** mapping:\n",
    "$$x \\xrightarrow{\\text{encode}} (\\mu, \\sigma^2) \\xrightarrow{\\text{sample}} z \\xrightarrow{\\text{decode}} \\hat{x}$$\n",
    "\n",
    "The KL regularization term encourages the latent distribution to be close to $\\mathcal{N}(0, I)$, ensuring:\n",
    "1. **Smooth latent space**: Nearby points decode to similar images\n",
    "2. **No holes**: The entire space is \"covered\" by the prior\n",
    "3. **Sampling works**: We can sample $z \\sim \\mathcal{N}(0, I)$ and decode to valid images\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "The encoder outputs $\\mu$ and $\\log\\sigma^2$ (log-variance for numerical stability).\n",
    "\n",
    "To sample $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ while allowing backpropagation:\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "This moves the randomness ($\\epsilon$) outside the computational graph, making $z$ differentiable with respect to $\\mu$ and $\\sigma$.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Image (32×32×3)                                    Image (32×32×3)\n",
    "      |                                                  ^\n",
    "      v                                                  |\n",
    "+------------+                                    +------------+\n",
    "|  Encoder   |  (downsample 4×)                   |  Decoder   |  (upsample 4×)\n",
    "+------------+                                    +------------+\n",
    "      |                                                  ^\n",
    "      v                                                  |\n",
    "  (μ, log σ²)  ───> sample z = μ + σε ───────────>    z\n",
    "   (8×8×8)              (8×8×4)                     (8×8×4)\n",
    "```\n",
    "\n",
    "Note: The encoder outputs 2× the latent channels (for $\\mu$ and $\\log\\sigma^2$), then we sample to get the final latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 for this demonstration\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset):,} images\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Pixel range: [{train_dataset[0][0].min():.1f}, {train_dataset[0][0].max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "def show_images(images, nrow=8, title=\"\"):\n",
    "    \"\"\"Display a grid of images.\"\"\"\n",
    "    images = (images + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    images = images.clamp(0, 1)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow, padding=2)\n",
    "    plt.figure(figsize=(12, 12 * grid.shape[1] / grid.shape[2]))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch and visualize\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "show_images(sample_batch[:32], title=\"CIFAR-10 Training Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vae-architecture",
   "metadata": {},
   "source": [
    "## 2. Building the VAE\n",
    "\n",
    "Our VAE architecture:\n",
    "\n",
    "**Encoder** (32×32 → 8×8, 4× spatial compression):\n",
    "- Input conv: 3 → 64 channels\n",
    "- Downsample block 1: 64 → 64, stride 2 (32→16)\n",
    "- Downsample block 2: 64 → 128, stride 2 (16→8)\n",
    "- Output conv: 128 → 8 (4 for μ, 4 for log σ²)\n",
    "\n",
    "**Decoder** (8×8 → 32×32):\n",
    "- Input conv: 4 → 128 channels\n",
    "- Upsample block 1: 128 → 64, upsample 2× (8→16)\n",
    "- Upsample block 2: 64 → 64, upsample 2× (16→32)\n",
    "- Output conv: 64 → 3\n",
    "\n",
    "**Compression Ratio**:\n",
    "- Pixel space: 32×32×3 = 3,072 values\n",
    "- Latent space: 8×8×4 = 256 values\n",
    "- **12× compression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-vae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.vae import SmallVAE\n",
    "\n",
    "# Create VAE for CIFAR-10 (32×32 RGB)\n",
    "vae = SmallVAE(\n",
    "    in_channels=3,        # RGB\n",
    "    latent_channels=4,    # 4 latent channels\n",
    "    hidden_channels=64,   # Base hidden channels\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print(f\"VAE parameters: {num_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_img = torch.randn(4, 3, 32, 32, device=device)\n",
    "with torch.no_grad():\n",
    "    recon, mean, logvar = vae(test_img)\n",
    "    latent = vae.encode(test_img)\n",
    "\n",
    "print(f\"\\nShape analysis:\")\n",
    "print(f\"  Input image:  {test_img.shape}  ({test_img.numel() // 4:,} values per image)\")\n",
    "print(f\"  Latent mean:  {mean.shape}  ({mean.numel() // 4:,} values per image)\")\n",
    "print(f\"  Latent z:     {latent.shape}\")\n",
    "print(f\"  Recon image:  {recon.shape}\")\n",
    "print(f\"\\nCompression ratio: {test_img.numel() / latent.numel():.1f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kl-math",
   "metadata": {},
   "source": [
    "## 3. The VAE Loss Function\n",
    "\n",
    "### Reconstruction Loss\n",
    "\n",
    "Simple MSE between input and reconstruction:\n",
    "\n",
    "$$\\mathcal{L}_{recon} = \\|x - \\hat{x}\\|^2 = \\|x - \\text{decode}(\\text{encode}(x))\\|^2$$\n",
    "\n",
    "### KL Divergence Loss\n",
    "\n",
    "The KL divergence between the encoder distribution and the prior:\n",
    "\n",
    "$$D_{KL}(q(z|x) \\| p(z)) = D_{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\| \\mathcal{N}(0, I))$$\n",
    "\n",
    "For Gaussians, this has a closed form:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2} \\sum_{i=1}^{d} \\left(1 + \\log\\sigma_i^2 - \\mu_i^2 - \\sigma_i^2\\right)$$\n",
    "\n",
    "**Intuition**:\n",
    "- If $\\mu \\to 0$ and $\\sigma \\to 1$: KL → 0 (matches prior)\n",
    "- If $\\mu$ is large: KL increases (penalizes mean far from 0)\n",
    "- If $\\sigma$ is very small or large: KL increases (penalizes variance different from 1)\n",
    "\n",
    "### Combining the Losses\n",
    "\n",
    "$$\\mathcal{L}_{VAE} = \\mathcal{L}_{recon} + \\beta \\cdot D_{KL}$$\n",
    "\n",
    "For **latent diffusion**, we use very small $\\beta$ (e.g., 0.00001) because:\n",
    "1. We prioritize reconstruction quality (the decoder needs to produce sharp images)\n",
    "2. The diffusion model will handle generation (so we don't need perfect sampling from the prior)\n",
    "3. We just need good encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understand-kl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KL divergence behavior\n",
    "def kl_divergence(mu, logvar):\n",
    "    \"\"\"KL divergence from N(mu, sigma^2) to N(0, 1).\"\"\"\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "# Test different scenarios\n",
    "scenarios = [\n",
    "    (\"μ=0, σ=1 (perfect match)\", 0.0, 0.0),\n",
    "    (\"μ=1, σ=1 (shifted mean)\", 1.0, 0.0),\n",
    "    (\"μ=0, σ=0.5 (narrow)\", 0.0, np.log(0.25)),\n",
    "    (\"μ=0, σ=2 (wide)\", 0.0, np.log(4.0)),\n",
    "    (\"μ=2, σ=0.5 (shifted+narrow)\", 2.0, np.log(0.25)),\n",
    "]\n",
    "\n",
    "print(\"KL Divergence Examples (per dimension):\")\n",
    "print(\"-\" * 50)\n",
    "for name, mu_val, logvar_val in scenarios:\n",
    "    mu = torch.tensor([mu_val])\n",
    "    logvar = torch.tensor([logvar_val])\n",
    "    kl = kl_divergence(mu, logvar).item()\n",
    "    print(f\"{name:35s}: KL = {kl:.4f}\")\n",
    "\n",
    "print(\"\\nThe KL term encourages μ→0 and σ→1 (standard normal).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-vae",
   "metadata": {},
   "source": [
    "## 4. Training the VAE\n",
    "\n",
    "### Training Algorithm\n",
    "\n",
    "For each batch:\n",
    "1. Forward pass: $x \\to (\\mu, \\log\\sigma^2) \\to z \\to \\hat{x}$\n",
    "2. Compute reconstruction loss: $\\mathcal{L}_{recon} = \\|x - \\hat{x}\\|^2$\n",
    "3. Compute KL loss: $\\mathcal{L}_{KL} = D_{KL}(q(z|x) \\| p(z))$\n",
    "4. Total loss: $\\mathcal{L} = \\mathcal{L}_{recon} + \\beta \\cdot \\mathcal{L}_{KL}$\n",
    "5. Backpropagate and update\n",
    "\n",
    "### $\\beta$ Schedule\n",
    "\n",
    "For latent diffusion, we use $\\beta \\approx 10^{-5}$. This extremely small value means:\n",
    "- Reconstruction quality is prioritized\n",
    "- Latents may not perfectly follow $\\mathcal{N}(0,I)$\n",
    "- But that's OK - we'll compute a scale factor to normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-vae-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.train import VAETrainer\n",
    "\n",
    "# Create trainer\n",
    "vae_trainer = VAETrainer(\n",
    "    model=vae,\n",
    "    dataloader=train_loader,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    kl_weight=0.00001,  # Very small β for latent diffusion\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Train VAE\n",
    "VAE_EPOCHS = 30  # Increase for better reconstruction\n",
    "print(f\"Training VAE for {VAE_EPOCHS} epochs...\")\n",
    "print(f\"KL weight (β): {vae_trainer.kl_weight}\")\n",
    "print()\n",
    "\n",
    "vae_losses = vae_trainer.train(num_epochs=VAE_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-vae-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot VAE training loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(vae_trainer.losses, label='Total Loss')\n",
    "axes[0].plot(vae_trainer.recon_losses, label='Reconstruction Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('VAE Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(vae_trainer.kl_losses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('KL Divergence')\n",
    "axes[1].set_title('KL Divergence over Training')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final reconstruction loss: {vae_trainer.recon_losses[-1]:.4f}\")\n",
    "print(f\"Final KL divergence: {vae_trainer.kl_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VAE reconstructions\n",
    "vae.eval()\n",
    "\n",
    "test_batch = sample_batch[:16].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon, _, _ = vae(test_batch)\n",
    "\n",
    "# Show original vs reconstruction\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 7))\n",
    "\n",
    "# Original images\n",
    "original_grid = torchvision.utils.make_grid((test_batch + 1) / 2, nrow=16, padding=2)\n",
    "axes[0].imshow(original_grid.permute(1, 2, 0).cpu().numpy())\n",
    "axes[0].set_title('Original Images', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Reconstructed images\n",
    "recon_grid = torchvision.utils.make_grid((recon + 1) / 2, nrow=16, padding=2)\n",
    "axes[1].imshow(recon_grid.permute(1, 2, 0).cpu().clamp(0, 1).numpy())\n",
    "axes[1].set_title('VAE Reconstructions', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute reconstruction error\n",
    "mse = F.mse_loss(recon, test_batch).item()\n",
    "print(f\"\\nReconstruction MSE: {mse:.4f}\")\n",
    "print(\"The VAE successfully compresses and reconstructs images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latent-viz",
   "metadata": {},
   "source": [
    "## 5. Exploring the Latent Space\n",
    "\n",
    "Let's visualize what the latent space looks like.\n",
    "\n",
    "### Latent Statistics\n",
    "\n",
    "After training, we compute a **scale factor** to normalize the latent space:\n",
    "\n",
    "$$z_{normalized} = z / \\sigma_{data}$$\n",
    "\n",
    "where $\\sigma_{data}$ is the empirical standard deviation of encoded latents.\n",
    "\n",
    "This ensures the latent space has approximately unit variance, matching the noise distribution $\\mathcal{N}(0, I)$ we'll use for flow matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latent-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze latent space statistics\n",
    "vae.eval()\n",
    "\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        if i >= 50:  # Use 50 batches for stats\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        z = vae.encode(images, sample=False)  # Use mean encoding\n",
    "        all_means.append(z.mean().item())\n",
    "        all_stds.append(z.std().item())\n",
    "\n",
    "avg_mean = np.mean(all_means)\n",
    "avg_std = np.mean(all_stds)\n",
    "\n",
    "print(f\"Latent Space Statistics:\")\n",
    "print(f\"  Mean: {avg_mean:.4f} (ideally ≈ 0)\")\n",
    "print(f\"  Std:  {avg_std:.4f} (will be normalized to ≈ 1)\")\n",
    "print(f\"\\nVAE scale factor: {vae.scale_factor.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent channels\n",
    "test_img = sample_batch[0:1].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = vae.encode(test_img, sample=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow((test_img[0].cpu().permute(1, 2, 0) + 1) / 2)\n",
    "axes[0].set_title('Original\\n(32×32×3)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Each latent channel\n",
    "for i in range(4):\n",
    "    latent_ch = z[0, i].cpu().numpy()\n",
    "    axes[i+1].imshow(latent_ch, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[i+1].set_title(f'Latent Ch {i}\\n(8×8)')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Image to Latent: 32×32×3 → 8×8×4 (12× compression)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLatent shape: {z.shape}\")\n",
    "print(f\"Latent values: min={z.min():.2f}, max={z.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latent-diffusion-intro",
   "metadata": {},
   "source": [
    "## 6. Flow Matching in Latent Space\n",
    "\n",
    "Now we train a DiT to perform flow matching **in the latent space** instead of pixel space.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Everything is the same as pixel-space flow matching, but we work with latents:\n",
    "\n",
    "| Pixel Space | Latent Space |\n",
    "|-------------|---------------|\n",
    "| $x_0$ = image | $z_0$ = encode(image) |\n",
    "| $x_1 \\sim \\mathcal{N}(0, I)$ | $z_1 \\sim \\mathcal{N}(0, I)$ |\n",
    "| $x_t = (1-t)x_0 + tx_1$ | $z_t = (1-t)z_0 + tz_1$ |\n",
    "| $v = x_1 - x_0$ | $v = z_1 - z_0$ |\n",
    "| Generate $x$ directly | Generate $z$, then decode |\n",
    "\n",
    "### DiT Architecture for Latent Space\n",
    "\n",
    "The DiT operates on the latent shape (8×8×4 for our VAE):\n",
    "- `in_channels=4` (latent channels)\n",
    "- `img_size=8` (latent spatial size)\n",
    "- `patch_size=2` (4×4 = 16 patches)\n",
    "\n",
    "This is **much smaller** than pixel-space DiT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-latent-dit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.dit import DiT\n",
    "\n",
    "# Create DiT for latent space\n",
    "# Latent shape: (4, 8, 8)\n",
    "latent_dit = DiT(\n",
    "    img_size=8,           # Latent spatial size\n",
    "    patch_size=2,         # 2×2 patches → 4×4 = 16 tokens\n",
    "    in_channels=4,        # Latent channels\n",
    "    embed_dim=256,        # Transformer embedding dimension\n",
    "    depth=6,              # Number of transformer blocks\n",
    "    num_heads=8,          # Attention heads\n",
    "    mlp_ratio=4.0,\n",
    ").to(device)\n",
    "\n",
    "latent_dit_params = sum(p.numel() for p in latent_dit.parameters())\n",
    "\n",
    "# Compare with pixel-space DiT\n",
    "pixel_dit = DiT(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    in_channels=3,\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    ")\n",
    "pixel_dit_params = sum(p.numel() for p in pixel_dit.parameters())\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"  Pixel-space DiT: {pixel_dit_params:,} params, {8*8}=64 tokens\")\n",
    "print(f\"  Latent-space DiT: {latent_dit_params:,} params, {4*4}=16 tokens\")\n",
    "print(f\"\\nToken reduction: {64/16:.0f}×\")\n",
    "print(f\"Attention cost reduction: {(64*64)/(16*16):.0f}×\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-latent-dit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.train import LatentDiffusionTrainer\n",
    "\n",
    "# Create latent diffusion trainer\n",
    "latent_trainer = LatentDiffusionTrainer(\n",
    "    model=latent_dit,\n",
    "    vae=vae,\n",
    "    dataloader=train_loader,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Train\n",
    "LATENT_EPOCHS = 30  # Increase for better results\n",
    "print(f\"Training Latent Diffusion for {LATENT_EPOCHS} epochs...\")\n",
    "print(f\"VAE is frozen - only DiT is trained.\")\n",
    "print()\n",
    "\n",
    "latent_losses = latent_trainer.train(num_epochs=LATENT_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-latent-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(latent_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Latent Diffusion Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {latent_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sampling-intro",
   "metadata": {},
   "source": [
    "## 7. Sampling from Latent Space\n",
    "\n",
    "Generation in latent diffusion:\n",
    "\n",
    "1. **Sample noise in latent space**: $z_1 \\sim \\mathcal{N}(0, I)$, shape (4, 8, 8)\n",
    "2. **Integrate ODE in latent space**: $z_1 \\to z_0$ using learned velocity\n",
    "3. **Decode to pixels**: $x = \\text{decode}(z_0)$, shape (3, 32, 32)\n",
    "\n",
    "### Why This Is Efficient\n",
    "\n",
    "All the expensive ODE integration happens in the small latent space:\n",
    "- 50 steps × 16 tokens = 800 attention operations per image\n",
    "- vs. 50 steps × 64 tokens = 3,200 operations in pixel space\n",
    "\n",
    "The VAE decode is just one forward pass at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.sampling import sample_latent\n",
    "\n",
    "# Generate samples\n",
    "latent_dit.eval()\n",
    "vae.eval()\n",
    "\n",
    "print(\"Generating samples via latent diffusion...\")\n",
    "print(\"  1. Sample noise in latent space (4×8×8)\")\n",
    "print(\"  2. Integrate ODE from t=1 to t=0\")\n",
    "print(\"  3. Decode to pixel space (3×32×32)\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = sample_latent(\n",
    "        model=latent_dit,\n",
    "        vae=vae,\n",
    "        num_samples=32,\n",
    "        latent_shape=(4, 8, 8),\n",
    "        num_steps=50,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "show_images(generated, nrow=8, title=\"Generated Samples (Latent Diffusion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-latent-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generation process in latent space\n",
    "with torch.no_grad():\n",
    "    generated, trajectory = sample_latent(\n",
    "        model=latent_dit,\n",
    "        vae=vae,\n",
    "        num_samples=4,\n",
    "        latent_shape=(4, 8, 8),\n",
    "        num_steps=50,\n",
    "        device=device,\n",
    "        return_trajectory=True,\n",
    "    )\n",
    "\n",
    "# Show evolution of one sample\n",
    "steps_to_show = [0, 10, 20, 30, 40, 50]\n",
    "fig, axes = plt.subplots(2, len(steps_to_show), figsize=(15, 6))\n",
    "\n",
    "for col, step_idx in enumerate(steps_to_show):\n",
    "    t_val = 1.0 - step_idx / 50\n",
    "    \n",
    "    # Latent (show first channel)\n",
    "    latent = trajectory[step_idx][0, 0].cpu().numpy()\n",
    "    axes[0, col].imshow(latent, cmap='RdBu', vmin=-3, vmax=3)\n",
    "    axes[0, col].set_title(f't={t_val:.2f}')\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    # Decoded image\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(trajectory[step_idx][:1])\n",
    "    img = (decoded[0].cpu().permute(1, 2, 0) + 1) / 2\n",
    "    axes[1, col].imshow(img.clamp(0, 1).numpy())\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Latent\\n(8×8)', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Decoded\\n(32×32)', fontsize=10)\n",
    "\n",
    "plt.suptitle('Latent Diffusion: ODE Integration + VAE Decode', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve how:\")\n",
    "print(\"  - t=1: Pure noise in latent space → noisy decoded image\")\n",
    "print(\"  - t→0: Structure emerges in latent → coherent image forms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-intro",
   "metadata": {},
   "source": [
    "## 8. Class-Conditional Latent Diffusion\n",
    "\n",
    "We can add class conditioning just like in Phase 3, but in latent space.\n",
    "\n",
    "The architecture is identical to `ConditionalDiT`, just operating on latent shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-conditional-latent-dit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.dit import ConditionalDiT\n",
    "\n",
    "# Create class-conditional DiT for latent space\n",
    "cond_latent_dit = ConditionalDiT(\n",
    "    num_classes=10,       # CIFAR-10 classes\n",
    "    img_size=8,           # Latent spatial size\n",
    "    patch_size=2,         # 2×2 patches\n",
    "    in_channels=4,        # Latent channels\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Conditional Latent DiT parameters: {sum(p.numel() for p in cond_latent_dit.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-conditional-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.train import LatentConditionalTrainer\n",
    "\n",
    "# Train conditional latent diffusion\n",
    "cond_latent_trainer = LatentConditionalTrainer(\n",
    "    model=cond_latent_dit,\n",
    "    vae=vae,\n",
    "    dataloader=train_loader,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    label_drop_prob=0.1,  # 10% dropout for CFG\n",
    "    num_classes=10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "COND_EPOCHS = 30\n",
    "print(f\"Training Conditional Latent Diffusion for {COND_EPOCHS} epochs...\")\n",
    "print()\n",
    "\n",
    "cond_losses = cond_latent_trainer.train(num_epochs=COND_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-conditional-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_image.sampling import sample_latent_conditional\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "cond_latent_dit.eval()\n",
    "\n",
    "# Generate one sample per class\n",
    "print(\"Generating one sample per class with CFG...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    class_samples = sample_latent_conditional(\n",
    "        model=cond_latent_dit,\n",
    "        vae=vae,\n",
    "        class_labels=list(range(10)),\n",
    "        latent_shape=(4, 8, 8),\n",
    "        num_steps=50,\n",
    "        cfg_scale=3.0,\n",
    "        device=device,\n",
    "        num_classes=10,\n",
    "    )\n",
    "\n",
    "# Display with class labels\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i, (ax, class_name) in enumerate(zip(axes.flat, CIFAR10_CLASSES)):\n",
    "    img = (class_samples[i].cpu().permute(1, 2, 0) + 1) / 2\n",
    "    ax.imshow(img.clamp(0, 1).numpy())\n",
    "    ax.set_title(class_name)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Class-Conditional Latent Diffusion (CFG=3.0)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfg-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CFG scales\n",
    "cfg_scales = [1.0, 2.0, 3.0, 5.0]\n",
    "target_class = 3  # cat\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cfg_scales), figsize=(16, 4))\n",
    "\n",
    "for ax, scale in zip(axes, cfg_scales):\n",
    "    torch.manual_seed(42)  # Same seed for comparison\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample = sample_latent_conditional(\n",
    "            model=cond_latent_dit,\n",
    "            vae=vae,\n",
    "            class_labels=[target_class],\n",
    "            latent_shape=(4, 8, 8),\n",
    "            num_steps=50,\n",
    "            cfg_scale=scale,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    img = (sample[0].cpu().permute(1, 2, 0) + 1) / 2\n",
    "    ax.imshow(img.clamp(0, 1).numpy())\n",
    "    ax.set_title(f'CFG={scale}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of CFG Scale (class: {CIFAR10_CLASSES[target_class]})', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Higher CFG → stronger class adherence but potentially less diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 9. Computational Comparison\n",
    "\n",
    "Let's compare the computational requirements of pixel-space vs latent-space diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timing-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Time comparison\n",
    "num_samples = 16\n",
    "num_steps = 50\n",
    "\n",
    "# Latent space timing\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = sample_latent(\n",
    "        model=latent_dit,\n",
    "        vae=vae,\n",
    "        num_samples=num_samples,\n",
    "        latent_shape=(4, 8, 8),\n",
    "        num_steps=num_steps,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "latent_time = time.time() - start\n",
    "\n",
    "print(f\"Latent Diffusion ({num_samples} samples, {num_steps} steps):\")\n",
    "print(f\"  Time: {latent_time:.2f}s\")\n",
    "print(f\"  Per image: {latent_time/num_samples*1000:.1f}ms\")\n",
    "print()\n",
    "print(\"Latent space details:\")\n",
    "print(f\"  Latent shape: (4, 8, 8) = 256 values\")\n",
    "print(f\"  Patches: 4×4 = 16 tokens\")\n",
    "print(f\"  Attention per step: 16×16 = 256 pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: The Complete Latent Diffusion Pipeline\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "TRAINING:\n",
    "                                                      Flow Matching Loss\n",
    "                                                             ↑\n",
    "Image x ───────> [VAE Encoder] ───> z_0 ───> z_t ───> [DiT] ───> v_pred\n",
    "                   (frozen)          ↑         ↑\n",
    "                                     |    noise z_1, time t\n",
    "                                     |\n",
    "                              scale_factor\n",
    "\n",
    "INFERENCE:\n",
    "Noise z_1 ───> [DiT + ODE] ───> z_0 ───> [VAE Decoder] ───> Image x\n",
    "```\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "| Component | Equation | Purpose |\n",
    "|-----------|----------|----------|\n",
    "| VAE Encode | $z = \\mu + \\sigma \\epsilon$ | Compress to latent |\n",
    "| VAE Decode | $\\hat{x} = D(z)$ | Reconstruct image |\n",
    "| VAE Loss | $\\mathcal{L} = \\|x-\\hat{x}\\|^2 + \\beta D_{KL}$ | Train VAE |\n",
    "| Latent Interpolation | $z_t = (1-t)z_0 + tz_1$ | Flow path |\n",
    "| Latent Velocity | $v = z_1 - z_0$ | Target |\n",
    "| Training Loss | $\\mathcal{L} = \\|v_\\theta(z_t, t) - v\\|^2$ | Train DiT |\n",
    "| Generation | $z_0 = z_1 - \\int_1^0 v_\\theta(z_t, t) dt$ | ODE solve |\n",
    "\n",
    "### Compression Ratios\n",
    "\n",
    "| Resolution | Pixel Dims | Latent Dims | Compression |\n",
    "|------------|------------|-------------|-------------|\n",
    "| 32×32×3 | 3,072 | 8×8×4 = 256 | 12× |\n",
    "| 64×64×3 | 12,288 | 8×8×4 = 256 | 48× |\n",
    "| 256×256×3 | 196,608 | 32×32×4 = 4,096 | 48× |\n",
    "| 512×512×3 | 786,432 | 64×64×4 = 16,384 | 48× |\n",
    "\n",
    "### Advantages of Latent Diffusion\n",
    "\n",
    "1. **Efficiency**: 48× fewer dimensions means faster training and inference\n",
    "2. **Scalability**: Can handle high-resolution images that would be infeasible in pixel space\n",
    "3. **Quality**: VAE provides perceptually meaningful compression\n",
    "4. **Flexibility**: Same DiT architecture works across resolutions\n",
    "\n",
    "### This Is How Stable Diffusion Works!\n",
    "\n",
    "The techniques in this notebook are the foundation of Stable Diffusion:\n",
    "- VAE compresses 512×512 images to 64×64 latents\n",
    "- U-Net (or DiT) does flow matching in latent space\n",
    "- CLIP text encoder provides conditioning\n",
    "- CFG provides strong text adherence\n",
    "\n",
    "Congratulations! You now understand the complete text-to-image pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "vae_trainer.save_checkpoint(\"phase5_vae.pt\")\n",
    "latent_trainer.save_checkpoint(\"phase5_latent_dit.pt\")\n",
    "cond_latent_trainer.save_checkpoint(\"phase5_cond_latent_dit.pt\")\n",
    "\n",
    "print(\"Models saved!\")\n",
    "print(\"  - phase5_vae.pt\")\n",
    "print(\"  - phase5_latent_dit.pt\")\n",
    "print(\"  - phase5_cond_latent_dit.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
